# ðŸ¤– Agente Conversacional RAG Local (CPU-Only)

![Python](https://img.shields.io/badge/Python-3.9-blue?style=for-the-badge&logo=python)
![Flask](https://img.shields.io/badge/Flask-2.0+-green?style=for-the-badge&logo=flask)
![LLM](https://img.shields.io/badge/Model-Qwen2.5--1.5B-orange?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Active-success?style=for-the-badge)

Este proyecto implementa un agente conversacional autÃ³nomo basado en arquitectura **RAG (Retrieval-Augmented Generation)** que se ejecuta enteramente en local. Combina modelos de lenguaje cuantizados (GGUF) con bÃºsqueda vectorial TF-IDF y lÃ³gica difusa para responder preguntas basadas en documentos `.txt` propios.

---

## ðŸ“‹ Tabla de Contenidos

1. [Requisitos Previos](#-requisitos-previos)
2. [InstalaciÃ³n del Entorno](#-instalaciÃ³n-del-entorno)
3. [ConfiguraciÃ³n del Modelo](#-configuraciÃ³n-del-modelo)
4. [Estructura del Proyecto](#-estructura-del-proyecto)
5. [EjecuciÃ³n](#-ejecuciÃ³n)

---

## ðŸ›  Requisitos Previos

* **Anaconda** o **Miniconda** instalado.
* Sistema Operativo: Windows, Linux o macOS.
* Memoria RAM: MÃ­nimo 4GB (Recomendado 8GB+).
* No requiere GPU dedicada.

---

## ðŸš€ InstalaciÃ³n del Entorno

Sigue estos pasos para configurar el entorno virtual y las dependencias necesarias.

### 1. Crear y activar el entorno
Utilizamos Python 3.9 para asegurar compatibilidad con las librerÃ­as de `llama-cpp`.

```bash
conda create -n chatbot_llm python=3.9
conda activate chatbot_llm
conda install jupyter  
pip install flask flask-cors nltk scikit-learn numpy rapidfuzz llama-cpp-python

mkdir models

/tu-proyecto
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ qwen2.5-1.5b-instruct-q4_k_m.gguf
â”‚
â”œâ”€â”€ knowledge_base/
â”‚   â”œâ”€â”€ documento1.txt
â”‚   â””â”€â”€ documento2.txt
â”‚
â”œâ”€â”€ app.py

python app.py